{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "print (\"Torch version:[%s]\"%(torch.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch2np(x_torch):\n",
    "    if x_torch is None:\n",
    "        x_np = None\n",
    "    else:\n",
    "        x_np = x_torch.detach().cpu().numpy()\n",
    "    return x_np\n",
    "def np2torch(x_np,device='cuda:0'):\n",
    "    if x_np is None:\n",
    "        x_torch = None\n",
    "    else:\n",
    "        x_torch = torch.tensor(x_np,dtype=torch.float32,device=device)\n",
    "    return x_torch\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(\n",
    "                self,\n",
    "                embedding_num   = 10,\n",
    "                embedding_dim   = 3,\n",
    "                commitment_beta = 0.25,\n",
    "                device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "                ):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.embedding_num   = embedding_num\n",
    "        self.embedding_dim   = embedding_dim\n",
    "        self.commitment_beta = commitment_beta\n",
    "        self.device    = device\n",
    "        self.embedding = nn.Embedding(self.embedding_num, self.embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/self.embedding_num, 1/self.embedding_num)\n",
    "        # torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        # self.embedding.weight.data.fill_(0)\n",
    "        # self.embedding.weight.data.fill_diagonal_(0.1)\n",
    "\n",
    "    def compute_loss(\n",
    "                    self,\n",
    "                    z_e,\n",
    "                    z_q\n",
    "                    ):\n",
    "        codebook_loss   = F.mse_loss(z_e.detach(), z_q)\n",
    "        commitment_loss = F.mse_loss(z_e, z_q.detach())\n",
    "        # return codebook_loss + self.commitment_beta*commitment_loss\n",
    "        return self.commitment_beta*commitment_loss\n",
    "\n",
    "    def forward(\n",
    "                self, \n",
    "                z = torch.randn(1, 15)\n",
    "                ):\n",
    "        z_dim = z.shape[1]\n",
    "        z = z.reshape(-1, int(z_dim/self.embedding_dim), self.embedding_dim)\n",
    "        z_e = z.view(-1, self.embedding_dim)\n",
    "        distances = torch.sum(z_e**2, dim=1, keepdim=True)\\\n",
    "                    + torch.sum(self.embedding.weight**2, dim=1, keepdim=False)\\\n",
    "                    - 2*torch.matmul(z_e, self.embedding.weight.t())\n",
    "        q_x = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        q_x_one_hot = torch.zeros(q_x.shape[0], self.embedding_num).to(self.device)\n",
    "        q_x_one_hot.scatter_(1, q_x, 1)\n",
    "        z_q  = torch.matmul(q_x_one_hot, self.embedding.weight).view(z.shape)\n",
    "        loss = self.compute_loss(z, z_q)\n",
    "        z_q = z + (z_q-z).detach()\n",
    "        return z_q.reshape(-1, z_dim), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizedVariationalAutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name     = 'VQVAE',              \n",
    "        x_dim    = 784,              # input dimension\n",
    "        z_dim    = 15,               # latent dimension\n",
    "        h_dims   = [64,32],          # hidden dimensions of encoder (and decoder)\n",
    "        embedding_num   = 10,        # For VQ parameters\n",
    "        embedding_dim   = 3,         # For VQ parameters\n",
    "        commitment_beta = 0.25,      # For VQ parameters\n",
    "        actv_enc = nn.ReLU(),        # encoder activation\n",
    "        actv_dec = nn.ReLU(),        # decoder activation\n",
    "        actv_out = None,             # output activation\n",
    "        var_max  = None,             # maximum variance\n",
    "        device   = 'cpu'\n",
    "        ):\n",
    "        \"\"\"\n",
    "            Initialize\n",
    "        \"\"\"\n",
    "        super(VectorQuantizedVariationalAutoEncoder, self).__init__()\n",
    "        self.name     = name\n",
    "        self.x_dim    = x_dim\n",
    "        self.z_dim    = z_dim\n",
    "        self.h_dims   = h_dims\n",
    "        self.embedding_num   = embedding_num\n",
    "        self.embedding_dim   = embedding_dim\n",
    "        self.commitment_beta = commitment_beta\n",
    "        self.actv_enc = actv_enc\n",
    "        self.actv_dec = actv_dec\n",
    "        self.actv_out = actv_out\n",
    "        self.var_max  = var_max\n",
    "        self.device   = device\n",
    "        # Initialize VQ class\n",
    "        self.VQ = VectorQuantizer(self.embedding_num, self.embedding_dim, self.commitment_beta).to(self.device)\n",
    "        # Initialize layers\n",
    "        self.init_layers()\n",
    "        self.init_params()\n",
    "                \n",
    "    def init_layers(self):\n",
    "        \"\"\"\n",
    "            Initialize layers\n",
    "        \"\"\"\n",
    "        self.layers = {}\n",
    "        \n",
    "        # Encoder part\n",
    "        h_dim_prev = self.x_dim\n",
    "        for h_idx,h_dim in enumerate(self.h_dims):\n",
    "            self.layers['enc_%02d_lin'%(h_idx)]  = \\\n",
    "                nn.Linear(h_dim_prev,h_dim,bias=True)\n",
    "            self.layers['enc_%02d_actv'%(h_idx)] = \\\n",
    "                self.actv_enc\n",
    "            h_dim_prev = h_dim\n",
    "        self.layers['z_lin']  = nn.Linear(h_dim_prev,self.z_dim,bias=True)\n",
    "        \n",
    "        # Decoder part\n",
    "        h_dim_prev = self.z_dim\n",
    "        for h_idx,h_dim in enumerate(self.h_dims[::-1]):\n",
    "            self.layers['dec_%02d_lin'%(h_idx)]  = \\\n",
    "                nn.Linear(h_dim_prev,h_dim,bias=True)\n",
    "            self.layers['dec_%02d_actv'%(h_idx)] = \\\n",
    "                self.actv_dec\n",
    "            h_dim_prev = h_dim\n",
    "        self.layers['out_lin'] = nn.Linear(h_dim_prev,self.x_dim,bias=True)\n",
    "        \n",
    "        # Append parameters\n",
    "        self.param_dict = {}\n",
    "        for key in self.layers.keys():\n",
    "            layer = self.layers[key]\n",
    "            if isinstance(layer,nn.Linear):\n",
    "                self.param_dict[key+'_w'] = layer.weight\n",
    "                self.param_dict[key+'_b'] = layer.bias\n",
    "        self.cvae_parameters = nn.ParameterDict(self.param_dict)\n",
    "        \n",
    "    def x_to_z(\n",
    "        self,\n",
    "        x = torch.randn(2,784)\n",
    "        ):\n",
    "        \"\"\"\n",
    "            x to z\n",
    "        \"\"\"\n",
    "        net = x\n",
    "        for h_idx,_ in enumerate(self.h_dims):\n",
    "            net = self.layers['enc_%02d_lin'%(h_idx)](net)\n",
    "            net = self.layers['enc_%02d_actv'%(h_idx)](net)\n",
    "        z = self.layers['z_lin'](net)\n",
    "        return z\n",
    "    \n",
    "    def z_to_x_recon(\n",
    "        self,\n",
    "        z\n",
    "        ):\n",
    "        \"\"\"\n",
    "            z and c to x_recon\n",
    "        \"\"\"\n",
    "        net, _ = self.VQ(z)\n",
    "        for h_idx,_ in enumerate(self.h_dims[::-1]):\n",
    "            net = self.layers['dec_%02d_lin'%(h_idx)](net)\n",
    "            net = self.layers['dec_%02d_actv'%(h_idx)](net)\n",
    "        net = self.layers['out_lin'](net)\n",
    "        if self.actv_out is not None:\n",
    "            net = self.actv_out(net)\n",
    "        x_recon = net\n",
    "        return x_recon\n",
    "\n",
    "    def z_q_to_x_recon(\n",
    "        self,\n",
    "        z_q\n",
    "        ):\n",
    "        \"\"\"\n",
    "            z and c to x_recon\n",
    "        \"\"\"\n",
    "        net = z_q\n",
    "        for h_idx,_ in enumerate(self.h_dims[::-1]):\n",
    "            net = self.layers['dec_%02d_lin'%(h_idx)](net)\n",
    "            net = self.layers['dec_%02d_actv'%(h_idx)](net)\n",
    "        net = self.layers['out_lin'](net)\n",
    "        if self.actv_out is not None:\n",
    "            net = self.actv_out(net)\n",
    "        x_recon = net\n",
    "        return x_recon\n",
    "\n",
    "    def x_to_x_recon(\n",
    "        self,\n",
    "        x = torch.randn(2,784),\n",
    "        ):\n",
    "        \"\"\"\n",
    "            x to x_recon\n",
    "        \"\"\"\n",
    "        z = self.x_to_z(x=x)\n",
    "        x_recon = self.z_to_x_recon(z=z)\n",
    "        return x_recon\n",
    "    \n",
    "    def init_params(self,seed=0):\n",
    "        \"\"\"\n",
    "            Initialize parameters\n",
    "        \"\"\"\n",
    "        # Fix random seed\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        # Init\n",
    "        for key in self.layers.keys():\n",
    "            layer = self.layers[key]\n",
    "            if isinstance(layer,nn.Linear):\n",
    "                nn.init.normal_(layer.weight,mean=0.0,std=0.01)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "            elif isinstance(layer,nn.BatchNorm2d):\n",
    "                nn.init.constant_(layer.weight,1.0)\n",
    "                nn.init.constant_(layer.bias,0.0)\n",
    "            elif isinstance(layer,nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def loss_recon(\n",
    "        self,\n",
    "        x               = torch.randn(2,784),\n",
    "        LOSS_TYPE       = 'L1+L2',\n",
    "        recon_loss_gain = 1.0\n",
    "        ):\n",
    "        \"\"\"\n",
    "            Recon loss\n",
    "        \"\"\"\n",
    "        x_recon = self.x_to_x_recon(x=x)\n",
    "        if (LOSS_TYPE == 'L1') or (LOSS_TYPE == 'MAE'):\n",
    "            errs = torch.mean(torch.abs(x-x_recon),axis=1)\n",
    "        elif (LOSS_TYPE == 'L2') or (LOSS_TYPE == 'MSE'):\n",
    "            errs = torch.mean(torch.square(x-x_recon),axis=1)\n",
    "        elif (LOSS_TYPE == 'L1+L2') or (LOSS_TYPE == 'EN'):\n",
    "            errs = torch.mean(\n",
    "                0.5*(torch.abs(x-x_recon)+torch.square(x-x_recon)),axis=1)\n",
    "        else:\n",
    "            raise Exception(\"VAE:[%s] Unknown loss_type:[%s]\"%\n",
    "                            (self.name,LOSS_TYPE))\n",
    "        return recon_loss_gain*torch.mean(errs)\n",
    "        \n",
    "    def loss_total(\n",
    "        self,\n",
    "        x               = torch.randn(2,784),\n",
    "        LOSS_TYPE       = 'L1+L2',\n",
    "        recon_loss_gain = 1.0\n",
    "        ):\n",
    "        \"\"\"\n",
    "            Total loss\n",
    "        \"\"\"\n",
    "        loss_recon_out = self.loss_recon(\n",
    "            x               = x,\n",
    "            LOSS_TYPE       = LOSS_TYPE,\n",
    "            recon_loss_gain = recon_loss_gain\n",
    "        )\n",
    "        z = self.x_to_z(x)\n",
    "        _, loss_vq = self.VQ(z)\n",
    "        loss_total_out = loss_recon_out + loss_vq\n",
    "        info           = {'loss_total_out' : loss_total_out,\n",
    "                          'loss_recon_out' : loss_recon_out,\n",
    "                          'loss_vq'        : loss_vq}\n",
    "        return loss_total_out,info\n",
    "\n",
    "    def debug_plot_img(\n",
    "        self,\n",
    "        x_train_np     = np.zeros((60000,784)),  # to plot encoded latent space \n",
    "        y_train_np     = np.zeros((60000)),      # to plot encoded latent space \n",
    "        c_train_np     = np.zeros((60000,10)),   # to plot encoded latent space\n",
    "        x_test_np      = np.zeros((10000,784)),\n",
    "        c_test_np      = np.zeros((10000,10)),\n",
    "        c_vecs         = np.eye(10,10),\n",
    "        n_sample       = 10,\n",
    "        img_shape      = (28,28),\n",
    "        img_cmap       = 'gray',\n",
    "        figsize_image  = (10,3.25),\n",
    "        figsize_latent = (10,3.25),\n",
    "        DPP_GEN        = False,\n",
    "        dpp_hyp        = {'g':1.0,'l':0.1}\n",
    "        ):\n",
    "        \"\"\"\n",
    "            Debug plot\n",
    "        \"\"\"\n",
    "        n_train       = x_train_np.shape[0]\n",
    "        x_train_torch = np2torch(x_train_np, device=self.device)\n",
    "        # Reconstruct\n",
    "        x_test_torch  = np2torch(x_test_np, device=self.device)\n",
    "        n_test        = x_test_np.shape[0]\n",
    "        rand_idxs     = np.random.permutation(n_test)[:n_sample]\n",
    "        x_recon = self.x_to_x_recon(x=x_test_torch[rand_idxs,:]).detach().cpu().numpy()\n",
    "        # Generation\n",
    "        random_integers  = np.random.permutation(self.embedding_num)[:n_sample]\n",
    "        random_embedding = self.VQ.embedding.weight.data[random_integers, :]\n",
    "        x_sample = self.z_q_to_x_recon(z_q=random_embedding).detach().cpu().numpy()\n",
    "        # Plot images to reconstruct\n",
    "        fig = plt.figure(figsize=figsize_image)\n",
    "        for s_idx in range(n_sample):\n",
    "            plt.subplot(1,n_sample,s_idx+1)\n",
    "            plt.imshow(x_test_np[rand_idxs[s_idx],:].reshape(img_shape),\n",
    "                       vmin=0,vmax=1,cmap=img_cmap)\n",
    "            plt.axis('off')\n",
    "        fig.suptitle(\"Images to Reconstruct\",fontsize=15);plt.show()\n",
    "        \n",
    "        # Plot reconstructed images\n",
    "        fig = plt.figure(figsize=figsize_image)\n",
    "        for s_idx in range(n_sample):\n",
    "            plt.subplot(1,n_sample,s_idx+1)\n",
    "            plt.imshow(x_recon[s_idx,:].reshape(img_shape),\n",
    "                       vmin=0,vmax=1,cmap=img_cmap)\n",
    "            plt.axis('off')\n",
    "        fig.suptitle(\"Reconstructed Images\",fontsize=15);plt.show()\n",
    "\n",
    "        # Plot generated images\n",
    "        fig = plt.figure(figsize=figsize_image)\n",
    "        for s_idx in range(n_sample):\n",
    "            plt.subplot(1,n_sample,s_idx+1)\n",
    "            plt.imshow(x_sample[s_idx,:].reshape(img_shape),\n",
    "                       vmin=0,vmax=1,cmap=img_cmap)\n",
    "            plt.axis('off')\n",
    "        fig.suptitle(\"Generated Images\",fontsize=15);plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets,transforms\n",
    "mnist_train = datasets.MNIST(\n",
    "    root='data/',train=True,transform=transforms.ToTensor(),download=True)\n",
    "mnist_test  = datasets.MNIST(\n",
    "    root='data/',train=False,transform=transforms.ToTensor(),download=True)\n",
    "# Training data\n",
    "x_train_torch               = mnist_train.data.float().reshape(-1,784)/255.\n",
    "y_train_torch               = mnist_train.targets\n",
    "x_test_torch                = mnist_test.data.float().reshape(-1,784)/255.\n",
    "y_test_torch                = mnist_test.targets\n",
    "x_train_np,x_test_np        = torch2np(x_train_torch),torch2np(x_test_torch)\n",
    "y_train_np,y_test_np        = torch2np(y_train_torch),torch2np(y_test_torch)\n",
    "c_train_np,c_test_np        = np.eye(10,10)[y_train_np],np.eye(10,10)[y_test_np]\n",
    "n_train,n_test              = x_train_np.shape[0],x_test_np.shape[0]\n",
    "c_train_torch,c_test_torch  = np2torch(c_train_np),np2torch(c_test_np)\n",
    "print (\"x_train_np:%s x_test_np:%s\"%(x_train_np.shape,x_test_np.shape,))\n",
    "print (\"c_train_np:%s c_test_np:%s\"%(c_train_np.shape,c_test_np.shape,))\n",
    "print (\"n_train:[%d] n_test:[%d]\"%(n_train,n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sunghyun/Project/snapbot-learning-v2/demo_vqvae_test.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e6f697379227d/home/sunghyun/Project/snapbot-learning-v2/demo_vqvae_test.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# Update\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e6f697379227d/home/sunghyun/Project/snapbot-learning-v2/demo_vqvae_test.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m total_loss_out,loss_info \u001b[39m=\u001b[39m VQVAE\u001b[39m.\u001b[39mloss_total(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e6f697379227d/home/sunghyun/Project/snapbot-learning-v2/demo_vqvae_test.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     x               \u001b[39m=\u001b[39m x_batch_torch,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e6f697379227d/home/sunghyun/Project/snapbot-learning-v2/demo_vqvae_test.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     LOSS_TYPE       \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mL1+L2\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e6f697379227d/home/sunghyun/Project/snapbot-learning-v2/demo_vqvae_test.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     recon_loss_gain \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e6f697379227d/home/sunghyun/Project/snapbot-learning-v2/demo_vqvae_test.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m optm\u001b[39m.\u001b[39mzero_grad(); total_loss_out\u001b[39m.\u001b[39mbackward(); optm\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e6f697379227d/home/sunghyun/Project/snapbot-learning-v2/demo_vqvae_test.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Errors\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e6f697379227d/home/sunghyun/Project/snapbot-learning-v2/demo_vqvae_test.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m loss_total_sum \u001b[39m=\u001b[39m loss_total_sum \u001b[39m+\u001b[39m n_batch\u001b[39m*\u001b[39mloss_info[\u001b[39m'\u001b[39m\u001b[39mloss_total_out\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/main/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/main/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/main/lib/python3.8/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[1;32m    139\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 141\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[1;32m    142\u001b[0m            grads,\n\u001b[1;32m    143\u001b[0m            exp_avgs,\n\u001b[1;32m    144\u001b[0m            exp_avg_sqs,\n\u001b[1;32m    145\u001b[0m            max_exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m            state_steps,\n\u001b[1;32m    147\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    148\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    149\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    150\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    151\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m            maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.pyenv/versions/main/lib/python3.8/site-packages/torch/optim/_functional.py:105\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    103\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(bias_correction2))\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[1;32m    109\u001b[0m step_size \u001b[39m=\u001b[39m lr \u001b[39m/\u001b[39m bias_correction1\n\u001b[1;32m    110\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "VQVAE = VectorQuantizedVariationalAutoEncoder(\n",
    "                                            name     = 'VQVAE',\n",
    "                                            x_dim    = 784,\n",
    "                                            z_dim    = 32,\n",
    "                                            h_dims   = [256,256],\n",
    "                                            embedding_num   = 300,\n",
    "                                            embedding_dim   = 32,\n",
    "                                            commitment_beta = 0.25,\n",
    "                                            actv_enc = nn.ReLU(),\n",
    "                                            actv_dec = nn.ReLU(),\n",
    "                                            actv_out = None,\n",
    "                                            var_max  = None,\n",
    "                                            device   ='cuda:0'\n",
    "                                            )\n",
    "print(VQVAE.VQ.embedding.weight.data)\n",
    "VQVAE = VQVAE.to(VQVAE.device)\n",
    "print (\"[{}] instantiated with device:[{}]\".format(VQVAE.name, VQVAE.device))\n",
    "# Hyperparameters\n",
    "n_epoch,print_every,batch_size,plot_every = 100,1,128,5\n",
    "n_it = (n_train//batch_size) + 1 # number of iterations\n",
    "optm = torch.optim.Adam(\n",
    "    params = VQVAE.parameters(),\n",
    "    lr     = 0.001,\n",
    "    betas  = (0.9,0.99),  # (0.9, 0.999)\n",
    "    eps    = 1e-8        # 1e-8\n",
    ") \n",
    "dpi = {'x_train_np':x_train_np,'y_train_np':y_train_np,'c_train_np':c_train_np,\n",
    "       'x_test_np':x_test_np,'c_test_np':c_test_np,'c_vecs':np.eye(10,10),\n",
    "       'figsize_image':(10,1.25),'figsize_latent':(10,3.25),\n",
    "       'DPP_GEN':True,'dpp_hyp':{'g':1.0,'l':1.0}}\n",
    "# C.debug_plot_img(**dpi)\n",
    "print (\"Start training.\")\n",
    "x_train_torch = x_train_torch.to(VQVAE.device)\n",
    "c_train_torch = c_train_torch.to(VQVAE.device)\n",
    "for epoch in range(n_epoch):\n",
    "    zero_to_one = float(epoch/(n_epoch-1))\n",
    "    loss_recon_sum,loss_vq_sum,loss_total_sum,n_batch_sum = 0.0,0.0,0.0,0\n",
    "    rand_idxs = np.random.permutation(n_train)\n",
    "    for it in range(n_it):\n",
    "        batch_idx     = rand_idxs[it*batch_size:(it+1)*batch_size]\n",
    "        x_batch_torch = x_train_torch[batch_idx,:]\n",
    "        c_batch_torch = c_train_torch[batch_idx,:]\n",
    "        n_batch       = x_batch_torch.shape[0]\n",
    "        # Update\n",
    "        total_loss_out,loss_info = VQVAE.loss_total(\n",
    "            x               = x_batch_torch,\n",
    "            LOSS_TYPE       = 'L1+L2',\n",
    "            recon_loss_gain = 1.0)\n",
    "        optm.zero_grad(); total_loss_out.backward(); optm.step()\n",
    "        # Errors\n",
    "        loss_total_sum = loss_total_sum + n_batch*loss_info['loss_total_out']\n",
    "        loss_recon_sum = loss_recon_sum + n_batch*loss_info['loss_recon_out']\n",
    "        loss_vq_sum    = loss_vq_sum + n_batch*loss_info['loss_vq']\n",
    "        n_batch_sum    = n_batch_sum + n_batch\n",
    "    loss_total_avg = loss_total_sum / n_batch_sum\n",
    "    loss_recon_avg = loss_recon_sum / n_batch_sum\n",
    "    loss_vq_avg    = loss_vq_sum / n_batch_sum\n",
    "    \n",
    "    # Print\n",
    "    print (\"[%d/%d] total:[%.3f] recon:[%.3f] vq:[%.3f]\"%\n",
    "           (epoch,n_epoch,loss_total_avg,loss_recon_avg, 10000*loss_vq_avg))\n",
    "    \n",
    "    # Plot test images\n",
    "    if ((epoch%plot_every) == 0) or (epoch==(n_epoch-1)):\n",
    "        VQVAE.debug_plot_img(**dpi)\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VQVAE.VQ.embedding.weight.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cov(VQVAE.VQ.embedding.weight.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c269583280d8b10072b2ac0e98618256c045871c10e4998e869cd86e2e007f5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
