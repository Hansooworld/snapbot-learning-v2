{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapbot(4legs) Environment\n",
      "Obs Dim: [103] Act Dim: [8] dt:[0.02] Condition:[False]\n",
      "ctrl_coef:[0] body_coef:[0] jump_coef:[0] vel_coef:[0] head_coef:[0]\n",
      "x_train shape: torch.Size([1000, 40])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "from class_snapbot import Snapbot4EnvClass\n",
    "from class_dlpg import DeepLatentPolicyGradientClass\n",
    "\n",
    "env        = Snapbot4EnvClass(VERBOSE=True, condition=False, render_mode=None)\n",
    "PriorDLPG  = DeepLatentPolicyGradientClass(\n",
    "                                        name     = 'PriorDLPG',              \n",
    "                                        x_dim    = 160,              # input dimension\n",
    "                                        c_dim    = 3,               # condition dimension\n",
    "                                        z_dim    = 32,               # latent dimension\n",
    "                                        h_dims   = [128, 128],          # hidden dimensions of encoder (and decoder)\n",
    "                                        actv_enc = nn.ReLU(),        # encoder activation\n",
    "                                        actv_dec = nn.ReLU(),        # decoder activation\n",
    "                                        actv_q   = nn.Softplus(),    # q activation\n",
    "                                        actv_out = None,             # output activation\n",
    "                                        var_max  = -1,             # maximum variance\n",
    "                                        device   = 'cuda:0'\n",
    "                                        )\n",
    "PriorDLPG.to(PriorDLPG.device)\n",
    "PriorDLPG.load_state_dict(torch.load(\"dlpg/14/weights/dlpg_model_weights_200.pth\", map_location='cuda:0'))\n",
    "\n",
    "n_sample = 1000\n",
    "c = torch.zeros(size=(n_sample, 3)).to(PriorDLPG.device)\n",
    "c[:, 1] = 1\n",
    "\n",
    "x_train = PriorDLPG.sample_x(\n",
    "                            c             = c,\n",
    "                            n_sample      = n_sample,\n",
    "                            SKIP_Z_SAMPLE = True\n",
    "                            ).reshape(n_sample, 8, -1)\n",
    "\n",
    "x_train = x_train[:, :, ::4].detach()\n",
    "x_train = x_train.reshape(n_sample, -1)\n",
    "print(\"x_train shape: {}\".format(x_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class ConditionalVariationalAutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name     = 'DLPG',              \n",
    "        x_dim    = 784,              # input dimension\n",
    "        c_dim    = 10,               # condition dimension\n",
    "        z_dim    = 16,               # latent dimension\n",
    "        h_dims   = [64,32],          # hidden dimensions of encoder (and decoder)\n",
    "        actv_enc = nn.ReLU(),        # encoder activation\n",
    "        actv_dec = nn.ReLU(),        # decoder activation\n",
    "        actv_out = None,             # output activation\n",
    "        var_max  = None,             # maximum variance\n",
    "        device   = 'cpu'\n",
    "        ):\n",
    "        \"\"\"\n",
    "            Initialize\n",
    "        \"\"\"\n",
    "        super(ConditionalVariationalAutoEncoder,self).__init__()\n",
    "        self.name = name\n",
    "        self.x_dim    = x_dim\n",
    "        self.c_dim    = c_dim\n",
    "        self.z_dim    = z_dim\n",
    "        self.h_dims   = h_dims\n",
    "        self.actv_enc = actv_enc\n",
    "        self.actv_dec = actv_dec\n",
    "        self.actv_out = actv_out\n",
    "        self.var_max  = var_max\n",
    "        self.device   = device\n",
    "        # Initialize layers\n",
    "        self.init_layers()\n",
    "        self.init_params()\n",
    "                \n",
    "    def init_layers(self):\n",
    "        \"\"\"\n",
    "            Initialize layers\n",
    "        \"\"\"\n",
    "        self.layers = {}\n",
    "        \n",
    "        # Encoder part\n",
    "        h_dim_prev = self.x_dim + self.c_dim\n",
    "        for h_idx,h_dim in enumerate(self.h_dims):\n",
    "            self.layers['enc_%02d_lin'%(h_idx)]  = \\\n",
    "                nn.Linear(h_dim_prev,h_dim,bias=True)\n",
    "            self.layers['enc_%02d_actv'%(h_idx)] = \\\n",
    "                self.actv_enc\n",
    "            h_dim_prev = h_dim\n",
    "        self.layers['z_mu_lin']  = nn.Linear(h_dim_prev,self.z_dim,bias=True)\n",
    "        self.layers['z_var_lin'] = nn.Linear(h_dim_prev,self.z_dim,bias=True)\n",
    "        \n",
    "        # Decoder part\n",
    "        h_dim_prev = self.z_dim + self.c_dim\n",
    "        for h_idx,h_dim in enumerate(self.h_dims[::-1]):\n",
    "            self.layers['dec_%02d_lin'%(h_idx)]  = \\\n",
    "                nn.Linear(h_dim_prev,h_dim,bias=True)\n",
    "            self.layers['dec_%02d_actv'%(h_idx)] = \\\n",
    "                self.actv_dec\n",
    "            h_dim_prev = h_dim\n",
    "        self.layers['out_lin'] = nn.Linear(h_dim_prev,self.x_dim,bias=True)\n",
    "        \n",
    "        # Append parameters\n",
    "        self.param_dict = {}\n",
    "        for key in self.layers.keys():\n",
    "            layer = self.layers[key]\n",
    "            if isinstance(layer,nn.Linear):\n",
    "                self.param_dict[key+'_w'] = layer.weight\n",
    "                self.param_dict[key+'_b'] = layer.bias\n",
    "        self.cvae_parameters = nn.ParameterDict(self.param_dict)\n",
    "        \n",
    "    def xc_to_z_mu(\n",
    "        self,\n",
    "        x = torch.randn(2,784),\n",
    "        c = torch.randn(2,10)\n",
    "        ):\n",
    "        \"\"\"\n",
    "            x and c to z_mu\n",
    "        \"\"\"\n",
    "        if c is not None:\n",
    "            net = torch.cat((x,c),dim=1)\n",
    "        else:\n",
    "            net = x\n",
    "        for h_idx,_ in enumerate(self.h_dims):\n",
    "            net = self.layers['enc_%02d_lin'%(h_idx)](net)\n",
    "            net = self.layers['enc_%02d_actv'%(h_idx)](net)\n",
    "        z_mu = self.layers['z_mu_lin'](net)\n",
    "        return z_mu\n",
    "    \n",
    "    def xc_to_h(\n",
    "        self,\n",
    "        x = torch.randn(2,784),\n",
    "        c = torch.randn(2,10)\n",
    "        ):\n",
    "        \"\"\"\n",
    "            x and c to z_mu\n",
    "        \"\"\"\n",
    "        hidden_value = []\n",
    "        if c is not None:\n",
    "            net = torch.cat((x,c),dim=1)\n",
    "        else:\n",
    "            net = x\n",
    "        for h_idx,_ in enumerate(self.h_dims):\n",
    "            net = self.layers['enc_%02d_lin'%(h_idx)](net)\n",
    "            # hidden_value.append(net.detach())\n",
    "            net = self.layers['enc_%02d_actv'%(h_idx)](net)\n",
    "            hidden_value.append(net.detach())\n",
    "        return hidden_value\n",
    "\n",
    "    def xc_to_z_var(\n",
    "        self,\n",
    "        x = torch.randn(2,784),\n",
    "        c = torch.randn(2,10)\n",
    "        ):\n",
    "        \"\"\"\n",
    "            x and c to z_var\n",
    "        \"\"\"\n",
    "        if c is not None:\n",
    "            net = torch.cat((x,c),dim=1)\n",
    "        else:\n",
    "            net = x\n",
    "        for h_idx,_ in enumerate(self.h_dims):\n",
    "            net = self.layers['enc_%02d_lin'%(h_idx)](net)\n",
    "            net = self.layers['enc_%02d_actv'%(h_idx)](net)\n",
    "        net = self.layers['z_var_lin'](net)\n",
    "        if self.var_max is None:\n",
    "            net = torch.exp(net)\n",
    "        else:\n",
    "            net = self.var_max*torch.sigmoid(net)\n",
    "        z_var = net\n",
    "        return z_var\n",
    "    \n",
    "    def zc_to_x_recon(\n",
    "        self,\n",
    "        z = torch.randn(2,16),\n",
    "        c = torch.randn(2,10)\n",
    "        ):\n",
    "        \"\"\"\n",
    "            z and c to x_recon\n",
    "        \"\"\"\n",
    "        if c is not None:\n",
    "            net = torch.cat((z,c),dim=1)\n",
    "        else:\n",
    "            net = z\n",
    "        for h_idx,_ in enumerate(self.h_dims[::-1]):\n",
    "            net = self.layers['dec_%02d_lin'%(h_idx)](net)\n",
    "            net = self.layers['dec_%02d_actv'%(h_idx)](net)\n",
    "        net = self.layers['out_lin'](net)\n",
    "        if self.actv_out is not None:\n",
    "            net = self.actv_out(net)\n",
    "        x_recon = net\n",
    "        return x_recon\n",
    "    \n",
    "    def xc_to_z_sample(\n",
    "        self,\n",
    "        x = torch.randn(2,784),\n",
    "        c = torch.randn(2,10)\n",
    "        ):\n",
    "        \"\"\"\n",
    "            x and c to z_sample\n",
    "        \"\"\"\n",
    "        z_mu,z_var = self.xc_to_z_mu(x=x,c=c),self.xc_to_z_var(x=x,c=c)\n",
    "        eps_sample = torch.randn(\n",
    "            size=z_mu.shape,dtype=torch.float32).to(self.device)\n",
    "        z_sample   = z_mu + torch.sqrt(z_var+1e-10)*eps_sample\n",
    "        return z_sample\n",
    "    \n",
    "    def xc_to_x_recon(\n",
    "        self,\n",
    "        x             = torch.randn(2,784),\n",
    "        c             = torch.randn(2,10), \n",
    "        STOCHASTICITY = True\n",
    "        ):\n",
    "        \"\"\"\n",
    "            x and c to x_recon\n",
    "        \"\"\"\n",
    "        if STOCHASTICITY:\n",
    "            z_sample = self.xc_to_z_sample(x=x,c=c)\n",
    "        else:\n",
    "            z_sample = self.xc_to_z_mu(x=x,c=c)\n",
    "        x_recon = self.zc_to_x_recon(z=z_sample,c=c)\n",
    "        return x_recon\n",
    "    \n",
    "    def sample_x(\n",
    "        self,\n",
    "        c             = torch.randn(5,10),\n",
    "        n_sample      = 5,\n",
    "        SKIP_Z_SAMPLE = False\n",
    "        ):\n",
    "        \"\"\"\n",
    "            Sample x\n",
    "        \"\"\"\n",
    "        z_sample = torch.randn(\n",
    "            size=(n_sample,self.z_dim),dtype=torch.float32).to(self.device)\n",
    "        if SKIP_Z_SAMPLE:\n",
    "            return self.zc_to_x_recon(z=z_sample,c=c)\n",
    "        else:\n",
    "            return self.zc_to_x_recon(z=z_sample,c=c),z_sample\n",
    "    \n",
    "    def init_params(self,seed=0):\n",
    "        \"\"\"\n",
    "            Initialize parameters\n",
    "        \"\"\"\n",
    "        # Fix random seed\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        # Init\n",
    "        for key in self.layers.keys():\n",
    "            layer = self.layers[key]\n",
    "            if isinstance(layer,nn.Linear):\n",
    "                nn.init.normal_(layer.weight,mean=0.0,std=0.01)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "            elif isinstance(layer,nn.BatchNorm2d):\n",
    "                nn.init.constant_(layer.weight,1.0)\n",
    "                nn.init.constant_(layer.bias,0.0)\n",
    "            elif isinstance(layer,nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def loss_recon(\n",
    "        self,\n",
    "        x               = torch.randn(2,784),\n",
    "        c               = torch.randn(2,10),\n",
    "        LOSS_TYPE       = 'L1+L2',\n",
    "        recon_loss_gain = 1.0,\n",
    "        STOCHASTICITY   = True\n",
    "        ):\n",
    "        \"\"\"\n",
    "            Recon loss\n",
    "        \"\"\"\n",
    "        x_recon = self.xc_to_x_recon(x=x,c=c,STOCHASTICITY=STOCHASTICITY)\n",
    "        if (LOSS_TYPE == 'L1') or (LOSS_TYPE == 'MAE'):\n",
    "            errs = torch.mean(torch.abs(x-x_recon),axis=1)\n",
    "        elif (LOSS_TYPE == 'L2') or (LOSS_TYPE == 'MSE'):\n",
    "            errs = torch.mean(torch.square(x-x_recon),axis=1)\n",
    "        elif (LOSS_TYPE == 'L1+L2') or (LOSS_TYPE == 'EN'):\n",
    "            errs = torch.mean(\n",
    "                0.5*(torch.abs(x-x_recon)+torch.square(x-x_recon)),axis=1)\n",
    "        else:\n",
    "            raise Exception(\"VAE:[%s] Unknown loss_type:[%s]\"%\n",
    "                            (self.name,LOSS_TYPE))\n",
    "        return recon_loss_gain*torch.mean(errs)\n",
    "    \n",
    "    def loss_kl(\n",
    "        self,\n",
    "        x = torch.randn(2,784),\n",
    "        c = torch.randn(2,10)\n",
    "        ):\n",
    "        \"\"\"\n",
    "            KLD loss\n",
    "        \"\"\"\n",
    "        z_mu     = self.xc_to_z_mu(x=x,c=c)\n",
    "        z_var    = self.xc_to_z_var(x=x,c=c)\n",
    "        z_logvar = torch.log(z_var)\n",
    "        errs     = 0.5*torch.sum(z_var + z_mu**2 - 1.0 - z_logvar,axis=1)\n",
    "        return torch.mean(errs)\n",
    "        \n",
    "    def loss_total(\n",
    "        self,\n",
    "        x               = torch.randn(2,784),\n",
    "        c               = torch.randn(2,10),\n",
    "        LOSS_TYPE       = 'L1+L2',\n",
    "        recon_loss_gain = 1.0,\n",
    "        STOCHASTICITY   = True,\n",
    "        beta            = 1.0\n",
    "        ):\n",
    "        \"\"\"\n",
    "            Total loss\n",
    "        \"\"\"\n",
    "        loss_recon_out = self.loss_recon(\n",
    "            x               = x,\n",
    "            c               = c,\n",
    "            LOSS_TYPE       = LOSS_TYPE,\n",
    "            recon_loss_gain = recon_loss_gain,\n",
    "            STOCHASTICITY   = STOCHASTICITY\n",
    "        )\n",
    "        loss_kl_out    = beta*self.loss_kl(\n",
    "            x = x,\n",
    "            c = c\n",
    "        )\n",
    "        loss_total_out = loss_recon_out + loss_kl_out\n",
    "        info           = {'loss_recon_out' : loss_recon_out,\n",
    "                          'loss_kl_out'    : loss_kl_out,\n",
    "                          'loss_total_out' : loss_total_out,\n",
    "                          'beta'           : beta}\n",
    "        return loss_total_out,info\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        x  = torch.randn(2,784),\n",
    "        c  = torch.randn(2,10),\n",
    "        lr = 0.001,\n",
    "        max_iter   = 2400,\n",
    "        batch_size = 100\n",
    "        ):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        n_x       = x.shape[0]\n",
    "        loss_list = []\n",
    "        for n_iter in range(max_iter):\n",
    "            self.train()\n",
    "            rand_idx = np.random.permutation(n_x)[:batch_size]\n",
    "            x_batch  = x[rand_idx, :].to(self.device)\n",
    "            if c is not None:\n",
    "                c_batch = c[rand_idx, :].to(self.device)\n",
    "            else:\n",
    "                c_batch = None\n",
    "            total_loss, _ = self.loss_total(x=x_batch, c=c_batch, LOSS_TYPE='L2')\n",
    "            loss_list.append(total_loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (n_iter+1) % 10 == 0:\n",
    "                print(\"{}/{} Clear, Loss: {}\".format(n_iter+1, max_iter, total_loss.item()))\n",
    "        \n",
    "        return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/500 Clear, Loss: 1.7095259428024292\n",
      "20/500 Clear, Loss: 1.4097232818603516\n",
      "30/500 Clear, Loss: 1.406043529510498\n",
      "40/500 Clear, Loss: 1.404691219329834\n",
      "50/500 Clear, Loss: 1.4027942419052124\n",
      "60/500 Clear, Loss: 1.402838110923767\n",
      "70/500 Clear, Loss: 1.402674674987793\n",
      "80/500 Clear, Loss: 1.402874231338501\n",
      "90/500 Clear, Loss: 1.4036569595336914\n",
      "100/500 Clear, Loss: 1.4037344455718994\n",
      "110/500 Clear, Loss: 1.4028949737548828\n",
      "120/500 Clear, Loss: 1.4027738571166992\n",
      "130/500 Clear, Loss: 1.4026340246200562\n",
      "140/500 Clear, Loss: 1.402604103088379\n",
      "150/500 Clear, Loss: 1.4056934118270874\n",
      "160/500 Clear, Loss: 1.4037137031555176\n",
      "170/500 Clear, Loss: 1.4028632640838623\n",
      "180/500 Clear, Loss: 1.4026100635528564\n",
      "190/500 Clear, Loss: 1.4026598930358887\n",
      "200/500 Clear, Loss: 1.4026132822036743\n",
      "210/500 Clear, Loss: 1.4025967121124268\n",
      "220/500 Clear, Loss: 1.4028358459472656\n",
      "230/500 Clear, Loss: 1.402712106704712\n",
      "240/500 Clear, Loss: 1.402601957321167\n",
      "250/500 Clear, Loss: 1.4026130437850952\n",
      "260/500 Clear, Loss: 1.4026015996932983\n",
      "270/500 Clear, Loss: 1.4025893211364746\n",
      "280/500 Clear, Loss: 1.4025943279266357\n",
      "290/500 Clear, Loss: 1.4025901556015015\n",
      "300/500 Clear, Loss: 1.4025956392288208\n",
      "310/500 Clear, Loss: 1.4046058654785156\n",
      "320/500 Clear, Loss: 1.4027726650238037\n",
      "330/500 Clear, Loss: 1.4026647806167603\n",
      "340/500 Clear, Loss: 1.402633547782898\n",
      "350/500 Clear, Loss: 1.4026082754135132\n",
      "360/500 Clear, Loss: 1.4026007652282715\n",
      "370/500 Clear, Loss: 1.4025957584381104\n",
      "380/500 Clear, Loss: 1.4025914669036865\n",
      "390/500 Clear, Loss: 1.4025890827178955\n",
      "400/500 Clear, Loss: 1.4025897979736328\n",
      "410/500 Clear, Loss: 1.4025893211364746\n",
      "420/500 Clear, Loss: 1.4026143550872803\n",
      "430/500 Clear, Loss: 1.4066472053527832\n",
      "440/500 Clear, Loss: 1.4025942087173462\n",
      "450/500 Clear, Loss: 1.4025911092758179\n",
      "460/500 Clear, Loss: 1.4026408195495605\n",
      "470/500 Clear, Loss: 1.4026257991790771\n",
      "480/500 Clear, Loss: 1.402596354484558\n",
      "490/500 Clear, Loss: 1.4025894403457642\n",
      "500/500 Clear, Loss: 1.402590036392212\n"
     ]
    }
   ],
   "source": [
    "VAE  = ConditionalVariationalAutoEncoder(\n",
    "        name     = 'CVAE',              \n",
    "        x_dim    = 40,              # input dimension\n",
    "        c_dim    = 0,               # condition dimension\n",
    "        z_dim    = 2,               # latent dimension\n",
    "        h_dims   = [32, 32],          # hidden dimensions of encoder (and decoder)\n",
    "        actv_enc = nn.ReLU(),        # encoder activation\n",
    "        actv_dec = nn.ReLU(),        # decoder activation\n",
    "        actv_out = None,             # output activation\n",
    "        var_max  = 0.1,             # maximum variance\n",
    "        device   = 'cpu'\n",
    "        )\n",
    "loss = VAE.update(x=x_train, c=None, lr=0.01, max_iter=500, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3186, -0.0230, -0.3659, -0.0636, -0.3244, -0.0975, -0.1925, -0.2511,\n",
       "        -0.1959,  0.0181, -0.5489,  0.4296, -0.9305,  0.5267, -0.9182,  0.3270,\n",
       "        -0.6384, -0.2574, -0.4520, -0.5038, -0.3987, -0.5641, -0.5327, -0.1845,\n",
       "        -0.4308,  0.2725, -0.2853,  0.0968, -0.3812, -0.1691, -0.4881, -0.1950,\n",
       "        -0.3558,  0.0300, -0.1175,  0.1186, -0.2124,  0.0561, -0.3226, -0.0352],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAE.xc_to_x_recon(\n",
    "        x             = x_train[110].cpu(),\n",
    "        c             = None,\n",
    "        STOCHASTICITY = True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3184, -0.0233, -0.3655, -0.0642, -0.3229, -0.0995, -0.1925, -0.2505,\n",
       "        -0.1967,  0.0174, -0.5478,  0.4247, -0.9259,  0.5211, -0.9129,  0.3253,\n",
       "        -0.6349, -0.2561, -0.4483, -0.5023, -0.3952, -0.5650, -0.5300, -0.1869,\n",
       "        -0.4309,  0.2683, -0.2869,  0.0921, -0.3818, -0.1703, -0.4887, -0.1936,\n",
       "        -0.3573,  0.0313, -0.1180,  0.1168, -0.2125,  0.0555, -0.3228, -0.0354],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[721]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c269583280d8b10072b2ac0e98618256c045871c10e4998e869cd86e2e007f5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
